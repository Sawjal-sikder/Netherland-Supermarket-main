{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9220ddf3",
   "metadata": {},
   "source": [
    "# Dekamarkt Product Scraper v2.0\n",
    "## Sitemap-Based Approach with Detailed Product GraphQL\n",
    "\n",
    "This notebook implements a comprehensive dekamarkt scraper that:\n",
    "- **Discovers products** from sitemap: `https://www.dekamarkt.nl/products-sitemap.xml`\n",
    "- **Tracks lastmod dates** to scrape only updated products (like dirk.py)\n",
    "- **Uses detailed product GraphQL** to get complete product information\n",
    "- **Saves comprehensive data** including nutritional info, descriptions, images, etc.\n",
    "\n",
    "### Features:\n",
    "- âœ… Sitemap-based product discovery\n",
    "- âœ… LastMod date tracking for incremental updates\n",
    "- âœ… Detailed product information via GraphQL\n",
    "- âœ… Complete nutritional and ingredient data\n",
    "- âœ… Image URLs and product descriptions\n",
    "- âœ… CSV export with comprehensive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b1b1e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ IMPORTS AND CONFIGURATION\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# API Configuration\n",
    "ENDPOINT = 'https://web-deka-gateway.detailresult.nl/graphql'\n",
    "SITEMAP_URL = 'https://www.dekamarkt.nl/products-sitemap.xml'\n",
    "STORE_ID = 283  # Default store ID\n",
    "\n",
    "# Headers for GraphQL requests\n",
    "HEADERS = {\n",
    "    'accept': '*/*',\n",
    "    'accept-language': 'en-US,en;q=0.5',\n",
    "    'api_key': '6d3a42a3-6d93-4f98-838d-bcc0ab2307fd',\n",
    "    'content-type': 'application/json',\n",
    "    'origin': 'https://www.dekamarkt.nl',\n",
    "    'referer': 'https://www.dekamarkt.nl/',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Detailed product GraphQL query template\n",
    "PRODUCT_QUERY_TEMPLATE = '''{\n",
    "  \"query\": \"query { product(productId: %s) { productId headerText brand packaging isWeightProduct maxAmount department webgroup additionalDescription description declarations { contactInformation { contactName contactAdress } nutritionalInformation { standardPackagingUnit soldOrPrepared nutritionalValues { text value nutritionalSubValues { text value } } } storageInstructions cookingInstructions instructionsForUse ingredients allergiesInformation { text } } logos { position description image } images { image rankNumber mainImage } productAssortment(storeId: %s) { productId storeId normalPrice offerPrice isSingleUsePlastic productNumber startDate endDate productOffer { productId textPriceSign endDate startDate disclaimerStartDate disclaimerEndDate } productInformation { productId headerText subText packaging image isWeightProduct department webgroup brand logos { position description image } } } } }\",\n",
    "  \"variables\": {}\n",
    "}'''\n",
    "\n",
    "# File for storing last update tracking\n",
    "LAST_UPDATE_FILE = 'dekamarkt_last_update.pkl'\n",
    "\n",
    "print(\"âœ… Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2596835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sitemap parsing functions ready!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ—ºï¸ SITEMAP DISCOVERY FUNCTIONS\n",
    "\n",
    "def fetch_sitemap(url: str) -> str:\n",
    "    \"\"\"Fetch sitemap XML content from URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fetching sitemap: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_product_urls_from_sitemap(sitemap_content: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse sitemap XML to extract product IDs and lastmod dates.\n",
    "    Returns list of dicts with product_id, url, and lastmod.\n",
    "    \"\"\"\n",
    "    if not sitemap_content:\n",
    "        return []\n",
    "    \n",
    "    products = []\n",
    "    try:\n",
    "        root = ET.fromstring(sitemap_content)\n",
    "        \n",
    "        # Handle namespace\n",
    "        ns = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "        \n",
    "        for url_elem in root.findall('.//sitemap:url', ns):\n",
    "            loc_elem = url_elem.find('sitemap:loc', ns)\n",
    "            lastmod_elem = url_elem.find('sitemap:lastmod', ns)\n",
    "            \n",
    "            if loc_elem is not None:\n",
    "                url = loc_elem.text\n",
    "                # Extract product ID from URL (assuming format like /product/12345-product-name)\n",
    "                product_id_match = re.search(r'/product/(\\d+)', url)\n",
    "                \n",
    "                if product_id_match:\n",
    "                    product_id = product_id_match.group(1)\n",
    "                    lastmod = lastmod_elem.text if lastmod_elem is not None else None\n",
    "                    \n",
    "                    # Parse lastmod date if available\n",
    "                    lastmod_date = None\n",
    "                    if lastmod:\n",
    "                        try:\n",
    "                            # Handle different date formats\n",
    "                            if 'T' in lastmod:\n",
    "                                lastmod_date = datetime.fromisoformat(lastmod.replace('Z', '+00:00'))\n",
    "                            else:\n",
    "                                lastmod_date = datetime.strptime(lastmod, '%Y-%m-%d')\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    products.append({\n",
    "                        'product_id': product_id,\n",
    "                        'url': url,\n",
    "                        'lastmod': lastmod,\n",
    "                        'lastmod_date': lastmod_date\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error parsing sitemap: {e}\")\n",
    "    \n",
    "    return products\n",
    "\n",
    "def get_last_update_date() -> Optional[datetime]:\n",
    "    \"\"\"Get the last update date from storage.\"\"\"\n",
    "    if os.path.exists(LAST_UPDATE_FILE):\n",
    "        try:\n",
    "            with open(LAST_UPDATE_FILE, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        except:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def save_last_update_date(date: datetime):\n",
    "    \"\"\"Save the last update date to storage.\"\"\"\n",
    "    try:\n",
    "        with open(LAST_UPDATE_FILE, 'wb') as f:\n",
    "            pickle.dump(date, f)\n",
    "        print(f\"ğŸ“… Saved last update date: {date}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving last update date: {e}\")\n",
    "\n",
    "def filter_products_by_date(products: List[Dict], last_update: Optional[datetime] = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Filter products to only include those with lastmod after the last update date.\n",
    "    If no last update date, return all products.\n",
    "    \"\"\"\n",
    "    if not last_update:\n",
    "        print(\"ğŸ”„ No previous update date found - will scrape all products\")\n",
    "        return products\n",
    "    \n",
    "    filtered = []\n",
    "    for product in products:\n",
    "        if product['lastmod_date'] and product['lastmod_date'] > last_update:\n",
    "            filtered.append(product)\n",
    "    \n",
    "    print(f\"ğŸ“Š Found {len(filtered)} products updated after {last_update.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    return filtered\n",
    "\n",
    "print(\"âœ… Sitemap parsing functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6af8f57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Detailed product query functions ready!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ›’ DETAILED PRODUCT QUERY FUNCTIONS\n",
    "\n",
    "def fetch_product_details(product_id: str, store_id: int = STORE_ID) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch detailed product information using the comprehensive GraphQL query.\n",
    "    \"\"\"\n",
    "    query = PRODUCT_QUERY_TEMPLATE % (product_id, store_id)\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            ENDPOINT,\n",
    "            headers=HEADERS,\n",
    "            data=query,\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        if 'errors' in data:\n",
    "            print(f\"âŒ GraphQL errors for product {product_id}: {data['errors']}\")\n",
    "            return None\n",
    "            \n",
    "        product_data = data.get('data', {}).get('product')\n",
    "        if not product_data:\n",
    "            print(f\"âš ï¸ No product data found for ID {product_id}\")\n",
    "            return None\n",
    "            \n",
    "        return product_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fetching product {product_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_comprehensive_product_info(product_data: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract comprehensive product information from the detailed GraphQL response.\n",
    "    \"\"\"\n",
    "    if not product_data:\n",
    "        return {}\n",
    "    \n",
    "    # Basic product info\n",
    "    basic_info = {\n",
    "        'product_id': product_data.get('productId'),\n",
    "        'header_text': product_data.get('headerText'),\n",
    "        'brand': product_data.get('brand'),\n",
    "        'packaging': product_data.get('packaging'),\n",
    "        'is_weight_product': product_data.get('isWeightProduct'),\n",
    "        'max_amount': product_data.get('maxAmount'),\n",
    "        'department': product_data.get('department'),\n",
    "        'webgroup': product_data.get('webgroup'),\n",
    "        'description': product_data.get('description'),\n",
    "        'additional_description': product_data.get('additionalDescription')\n",
    "    }\n",
    "    \n",
    "    # Product assortment (pricing and offer info)\n",
    "    assortment = product_data.get('productAssortment', {})\n",
    "    if assortment:\n",
    "        basic_info.update({\n",
    "            'normal_price': assortment.get('normalPrice'),\n",
    "            'offer_price': assortment.get('offerPrice'),\n",
    "            'is_single_use_plastic': assortment.get('isSingleUsePlastic'),\n",
    "            'product_number': assortment.get('productNumber'),\n",
    "            'start_date': assortment.get('startDate'),\n",
    "            'end_date': assortment.get('endDate')\n",
    "        })\n",
    "        \n",
    "        # Product offer details\n",
    "        offer = assortment.get('productOffer', {})\n",
    "        if offer:\n",
    "            basic_info.update({\n",
    "                'offer_text_price_sign': offer.get('textPriceSign'),\n",
    "                'offer_start_date': offer.get('startDate'),\n",
    "                'offer_end_date': offer.get('endDate'),\n",
    "                'disclaimer_start_date': offer.get('disclaimerStartDate'),\n",
    "                'disclaimer_end_date': offer.get('disclaimerEndDate')\n",
    "            })\n",
    "    \n",
    "    # Images\n",
    "    images = product_data.get('images', [])\n",
    "    if images:\n",
    "        main_images = [img['image'] for img in images if img.get('mainImage')]\n",
    "        all_images = [img['image'] for img in images]\n",
    "        basic_info.update({\n",
    "            'main_image': main_images[0] if main_images else None,\n",
    "            'all_images': ', '.join(all_images) if all_images else None,\n",
    "            'image_count': len(all_images)\n",
    "        })\n",
    "    \n",
    "    # Logos\n",
    "    logos = product_data.get('logos', [])\n",
    "    if logos:\n",
    "        logo_descriptions = [logo.get('description', '') for logo in logos if logo.get('description')]\n",
    "        basic_info['logos'] = ', '.join(logo_descriptions) if logo_descriptions else None\n",
    "    \n",
    "    # Declarations (nutritional and other info)\n",
    "    declarations = product_data.get('declarations', {})\n",
    "    if declarations:\n",
    "        # Contact information\n",
    "        contact = declarations.get('contactInformation', {})\n",
    "        if contact:\n",
    "            basic_info.update({\n",
    "                'contact_name': contact.get('contactName'),\n",
    "                'contact_address': contact.get('contactAdress')\n",
    "            })\n",
    "        \n",
    "        # Nutritional information\n",
    "        nutrition = declarations.get('nutritionalInformation', {})\n",
    "        if nutrition:\n",
    "            basic_info.update({\n",
    "                'standard_packaging_unit': nutrition.get('standardPackagingUnit'),\n",
    "                'sold_or_prepared': nutrition.get('soldOrPrepared')\n",
    "            })\n",
    "            \n",
    "            # Nutritional values\n",
    "            nutritional_values = nutrition.get('nutritionalValues', [])\n",
    "            if nutritional_values:\n",
    "                nutrition_text = []\n",
    "                for nv in nutritional_values:\n",
    "                    text = nv.get('text', '')\n",
    "                    value = nv.get('value', '')\n",
    "                    if text and value:\n",
    "                        nutrition_text.append(f\"{text}: {value}\")\n",
    "                        \n",
    "                        # Sub-values\n",
    "                        sub_values = nv.get('nutritionalSubValues', [])\n",
    "                        for sv in sub_values:\n",
    "                            sub_text = sv.get('text', '')\n",
    "                            sub_value = sv.get('value', '')\n",
    "                            if sub_text and sub_value:\n",
    "                                nutrition_text.append(f\"  - {sub_text}: {sub_value}\")\n",
    "                \n",
    "                basic_info['nutritional_values'] = '; '.join(nutrition_text) if nutrition_text else None\n",
    "        \n",
    "        # Other declarations\n",
    "        basic_info.update({\n",
    "            'storage_instructions': declarations.get('storageInstructions'),\n",
    "            'cooking_instructions': declarations.get('cookingInstructions'),\n",
    "            'instructions_for_use': declarations.get('instructionsForUse'),\n",
    "            'ingredients': declarations.get('ingredients')\n",
    "        })\n",
    "        \n",
    "        # Allergies information\n",
    "        allergies = declarations.get('allergiesInformation', {})\n",
    "        if allergies and isinstance(allergies, dict):\n",
    "            basic_info['allergies_info'] = allergies.get('text')\n",
    "    \n",
    "    return basic_info\n",
    "\n",
    "print(\"âœ… Detailed product query functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "693787a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Main scraping orchestration ready!\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ MAIN SCRAPING ORCHESTRATION\n",
    "\n",
    "def scrape_dekamarkt_comprehensive(\n",
    "    store_id: int = STORE_ID,\n",
    "    delay_between_requests: float = 0.1,\n",
    "    max_products: int = None,\n",
    "    force_full_scrape: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Comprehensive Dekamarkt scraper using sitemap discovery and detailed product queries.\n",
    "    \n",
    "    Args:\n",
    "        store_id: Store ID for pricing/availability\n",
    "        delay_between_requests: Delay between API calls (seconds)\n",
    "        max_products: Maximum number of products to scrape (for testing)\n",
    "        force_full_scrape: If True, ignore lastmod dates and scrape all products\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with comprehensive product information\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ğŸ Starting Dekamarkt Comprehensive Scraper v2.0\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Fetch and parse sitemap\n",
    "    print(\"ğŸ“¥ Fetching product sitemap...\")\n",
    "    sitemap_content = fetch_sitemap(SITEMAP_URL)\n",
    "    if not sitemap_content:\n",
    "        print(\"âŒ Failed to fetch sitemap\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Step 2: Extract product URLs and lastmod dates\n",
    "    print(\"ğŸ” Parsing product URLs from sitemap...\")\n",
    "    products = parse_product_urls_from_sitemap(sitemap_content)\n",
    "    if not products:\n",
    "        print(\"âŒ No products found in sitemap\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"ğŸ“Š Found {len(products)} products in sitemap\")\n",
    "    \n",
    "    # Step 3: Filter by lastmod date (unless force_full_scrape)\n",
    "    if not force_full_scrape:\n",
    "        last_update = get_last_update_date()\n",
    "        products = filter_products_by_date(products, last_update)\n",
    "    else:\n",
    "        print(\"ğŸ”„ Force full scrape enabled - processing all products\")\n",
    "    \n",
    "    if not products:\n",
    "        print(\"âœ… No new products to scrape\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Step 4: Limit products for testing if specified\n",
    "    if max_products and len(products) > max_products:\n",
    "        products = products[:max_products]\n",
    "        print(f\"ğŸ¯ Limited to first {max_products} products for testing\")\n",
    "    \n",
    "    # Step 5: Scrape detailed product information\n",
    "    print(f\"ğŸ›’ Scraping detailed information for {len(products)} products...\")\n",
    "    scraped_products = []\n",
    "    errors = 0\n",
    "    \n",
    "    for i, product_info in enumerate(products, 1):\n",
    "        product_id = product_info['product_id']\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f\"ğŸ”„ Progress: {i}/{len(products)} products processed...\")\n",
    "        \n",
    "        # Fetch detailed product data\n",
    "        product_data = fetch_product_details(product_id, store_id)\n",
    "        if product_data:\n",
    "            # Extract comprehensive information\n",
    "            extracted_info = extract_comprehensive_product_info(product_data)\n",
    "            if extracted_info:\n",
    "                # Add sitemap metadata\n",
    "                extracted_info.update({\n",
    "                    'sitemap_url': product_info['url'],\n",
    "                    'lastmod': product_info['lastmod'],\n",
    "                    'scraped_at': datetime.now().isoformat()\n",
    "                })\n",
    "                scraped_products.append(extracted_info)\n",
    "            else:\n",
    "                errors += 1\n",
    "        else:\n",
    "            errors += 1\n",
    "        \n",
    "        # Rate limiting\n",
    "        if delay_between_requests > 0:\n",
    "            time.sleep(delay_between_requests)\n",
    "    \n",
    "    print(f\"âœ… Scraping completed!\")\n",
    "    print(f\"ğŸ“Š Successfully scraped: {len(scraped_products)} products\")\n",
    "    print(f\"âŒ Errors encountered: {errors} products\")\n",
    "    \n",
    "    # Step 6: Create DataFrame and save results\n",
    "    if scraped_products:\n",
    "        df = pd.DataFrame(scraped_products)\n",
    "        \n",
    "        # Save last update date\n",
    "        current_time = datetime.now()\n",
    "        save_last_update_date(current_time)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Results saved with {len(df)} products\")\n",
    "        print(f\"ğŸ“… Next scrape will check for products updated after: {current_time}\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"âŒ No products successfully scraped\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, filename: str = None) -> str:\n",
    "    \"\"\"Save DataFrame to CSV with timestamp.\"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"dekamarkt_products_comprehensive_{timestamp}.csv\"\n",
    "    \n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"ğŸ’¾ Saved {len(df)} products to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "print(\"âœ… Main scraping orchestration ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3769675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing Dekamarkt Comprehensive Scraper v2.0\n",
      "============================================================\n",
      "ğŸ“‹ Starting test run with limited products...\n",
      "ğŸ Starting Dekamarkt Comprehensive Scraper v2.0\n",
      "============================================================\n",
      "ğŸ“¥ Fetching product sitemap...\n",
      "ğŸ” Parsing product URLs from sitemap...\n",
      "âŒ No products found in sitemap\n",
      "âŒ Test failed - no data retrieved\n",
      "\n",
      "============================================================\n",
      "ğŸ§ª Test completed! Review results above.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª TEST THE NEW COMPREHENSIVE SCRAPER\n",
    "\n",
    "print(\"ğŸ§ª Testing Dekamarkt Comprehensive Scraper v2.0\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with a small sample first\n",
    "print(\"ğŸ“‹ Starting test run with limited products...\")\n",
    "\n",
    "# Run the comprehensive scraper\n",
    "df_comprehensive = scrape_dekamarkt_comprehensive(\n",
    "    store_id=283,\n",
    "    delay_between_requests=0.1,  # 100ms delay\n",
    "    max_products=5,  # Test with just 5 products first\n",
    "    force_full_scrape=True  # Ignore date filtering for test\n",
    ")\n",
    "\n",
    "if not df_comprehensive.empty:\n",
    "    print(f\"\\nğŸ“Š SAMPLE RESULTS OVERVIEW:\")\n",
    "    print(f\"Total products scraped: {len(df_comprehensive)}\")\n",
    "    print(f\"Columns available: {len(df_comprehensive.columns)}\")\n",
    "    print(f\"Sample columns: {list(df_comprehensive.columns[:10])}\")\n",
    "    \n",
    "    # Show a sample product\n",
    "    if len(df_comprehensive) > 0:\n",
    "        print(f\"\\nğŸ” SAMPLE PRODUCT DETAILS:\")\n",
    "        sample = df_comprehensive.iloc[0]\n",
    "        key_fields = ['product_id', 'header_text', 'brand', 'normal_price', 'offer_price', \n",
    "                     'department', 'description', 'nutritional_values', 'ingredients']\n",
    "        \n",
    "        for field in key_fields:\n",
    "            if field in sample and pd.notna(sample[field]):\n",
    "                value = str(sample[field])\n",
    "                if len(value) > 100:\n",
    "                    value = value[:100] + \"...\"\n",
    "                print(f\"  {field}: {value}\")\n",
    "    \n",
    "    # Show column summary\n",
    "    print(f\"\\nğŸ“‹ ALL AVAILABLE COLUMNS:\")\n",
    "    for i, col in enumerate(df_comprehensive.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ Test failed - no data retrieved\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ§ª Test completed! Review results above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e858b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Investigating sitemap structure...\n",
      "âœ… Sitemap fetched successfully (2148131 characters)\n",
      "\n",
      "ğŸ“‹ First 2000 characters of sitemap:\n",
      "------------------------------------------------------------\n",
      "<?xml version=\"1.0\" encoding=\"utf-8\"?><urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"><url><loc>https://www.dekamarkt.nl/boodschappen/producten/dranken-sap-koffie-thee/bier/heineken-pilsener-krat-24-x-300-ml/6</loc><lastmod>2024-10-03T00:55+02:00</lastmod></url><url><loc>https://www.dekamarkt.nl/boodschappen/producten/kind-drogisterij/bad-douche-zeep/trida-zeepblok-soft-en-sensitive-3-stuks/8</loc><lastmod>2024-10-03T00:55+02:00</lastmod></url><url><loc>https://www.dekamarkt.nl/boodschappen/producten/aardappelen-groente-fruit/groente/bio-biologische-rode-bieten-gekookt-500-g/9</loc><lastmod>2024-10-03T00:55+02:00</lastmod></url><url><loc>https://www.dekamarkt.nl/boodschappen/producten/voorraadkast/oliÃ«n-sauzen-mixen/maggi-doseerjus-naturel-180-g/10</loc><lastmod>2024-10-03T00:55+02:00</lastmod></url><url><loc>https://www.dekamarkt.nl/boodschappen/producten/dranken-sap-koffie-thee/aperitieven-mixen/tokkelroom-naturel-500-ml/11</loc><lastmod>2024-10-03T00:55+02:00</lastmod></url><url><loc>https://www.dekamarkt.nl/boodschappen/producten/voorraadkast/bakproducten-suiker/ambition-gembersiroop-300-ml/15</loc><lastmod>2024-10-03T00:55+02:00</lastmod></url><url><loc>https://www.dekamarkt.nl/boodschappen/producten/voorraadkast/bakproducten-suiker/ambition-chinese-bakgember-240-g/20</loc><lastmod>2024-10-03T00:55+02:00</lastmod></url><url><loc>https://www.dekamarkt.nl/boodschappen/producten/maaltijden-salades-tapas/verse-maaltijden/flemming-s-broodje-frikandel-2-stuks/28</loc><lastmod>2024-10-03T00:55+02:00</lastmod></url><url><loc>https://www.dekamarkt.nl/boodschappen/producten/huishoud-huisdieren/huisdieren/whiskas-saus-rundvlees-400-g/33</loc><lastmod>2024-10-03T00:55+02:00</lastmod></url><url><loc>https://www.dekamarkt.nl/boodschappen/producten/kind-drogisterij/babyverzorging/sudocrem-crÃ¨me-hypo-allergeen-125-g/37</loc><lastmod>2024-10-03T00:55+02:00</lastmod></url><url><loc>https://www.dekamarkt.nl/boodschappen/producten/brood-beleg-koek/broodbeleg/orinoko-ha\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ” XML Root tag: {http://www.sitemaps.org/schemas/sitemap/0.9}urlset\n",
      "ğŸ” XML Root attributes: {}\n",
      "âŒ Error parsing XML: module 'xml.etree.ElementTree' has no attribute 'StringIO'\n",
      "\n",
      "ğŸ” Searching for product URL patterns...\n",
      "Found 11611 URLs containing 'product'\n",
      "First 5 product URLs:\n",
      "  1. https://www.dekamarkt.nl/boodschappen/producten/dranken-sap-koffie-thee/bier/heineken-pilsener-krat-24-x-300-ml/6\n",
      "  2. https://www.dekamarkt.nl/boodschappen/producten/kind-drogisterij/bad-douche-zeep/trida-zeepblok-soft-en-sensitive-3-stuks/8\n",
      "  3. https://www.dekamarkt.nl/boodschappen/producten/aardappelen-groente-fruit/groente/bio-biologische-rode-bieten-gekookt-500-g/9\n",
      "  4. https://www.dekamarkt.nl/boodschappen/producten/voorraadkast/oliÃ«n-sauzen-mixen/maggi-doseerjus-naturel-180-g/10\n",
      "  5. https://www.dekamarkt.nl/boodschappen/producten/dranken-sap-koffie-thee/aperitieven-mixen/tokkelroom-naturel-500-ml/11\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” DEBUG SITEMAP STRUCTURE\n",
    "\n",
    "print(\"ğŸ” Investigating sitemap structure...\")\n",
    "\n",
    "# Fetch sitemap content\n",
    "sitemap_content = fetch_sitemap(SITEMAP_URL)\n",
    "\n",
    "if sitemap_content:\n",
    "    print(f\"âœ… Sitemap fetched successfully ({len(sitemap_content)} characters)\")\n",
    "    \n",
    "    # Show first 2000 characters to understand structure\n",
    "    print(\"\\nğŸ“‹ First 2000 characters of sitemap:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(sitemap_content[:2000])\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Try to parse XML structure\n",
    "    try:\n",
    "        root = ET.fromstring(sitemap_content)\n",
    "        print(f\"\\nğŸ” XML Root tag: {root.tag}\")\n",
    "        print(f\"ğŸ” XML Root attributes: {root.attrib}\")\n",
    "        \n",
    "        # Check namespaces\n",
    "        namespaces = {}\n",
    "        for event, elem in ET.iterparse(ET.StringIO(sitemap_content), events=(\"start-ns\",)):\n",
    "            namespaces[event] = elem\n",
    "        print(f\"ğŸ” Namespaces found: {namespaces}\")\n",
    "        \n",
    "        # List all direct children\n",
    "        print(f\"\\nğŸ” Direct children of root:\")\n",
    "        for i, child in enumerate(root):\n",
    "            print(f\"  {i+1}. Tag: {child.tag}, Attributes: {child.attrib}\")\n",
    "            if i < 5:  # Show details for first 5 elements\n",
    "                for subchild in child:\n",
    "                    print(f\"     - {subchild.tag}: {subchild.text[:100] if subchild.text else 'None'}\")\n",
    "            if i >= 10:  # Limit output\n",
    "                print(f\"  ... and {len(root) - 10} more elements\")\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error parsing XML: {e}\")\n",
    "        \n",
    "    # Look for product URLs manually\n",
    "    print(f\"\\nğŸ” Searching for product URL patterns...\")\n",
    "    product_matches = re.findall(r'<loc>([^<]*product[^<]*)</loc>', sitemap_content)\n",
    "    print(f\"Found {len(product_matches)} URLs containing 'product'\")\n",
    "    if product_matches:\n",
    "        print(\"First 5 product URLs:\")\n",
    "        for i, url in enumerate(product_matches[:5]):\n",
    "            print(f\"  {i+1}. {url}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Failed to fetch sitemap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b3129fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Testing fixed sitemap parsing...\n",
      "âœ… Successfully parsed 11611 products\n",
      "\n",
      "ğŸ“‹ Sample products:\n",
      "  1. ID: 6, LastMod: 2024-10-03T00:55+02:00\n",
      "     URL: https://www.dekamarkt.nl/boodschappen/producten/dranken-sap-koffie-thee/bier/heineken-pilsener-krat-24-x-300-ml/6\n",
      "  2. ID: 8, LastMod: 2024-10-03T00:55+02:00\n",
      "     URL: https://www.dekamarkt.nl/boodschappen/producten/kind-drogisterij/bad-douche-zeep/trida-zeepblok-soft-en-sensitive-3-stuks/8\n",
      "  3. ID: 9, LastMod: 2024-10-03T00:55+02:00\n",
      "     URL: https://www.dekamarkt.nl/boodschappen/producten/aardappelen-groente-fruit/groente/bio-biologische-rode-bieten-gekookt-500-g/9\n",
      "  4. ID: 10, LastMod: 2024-10-03T00:55+02:00\n",
      "     URL: https://www.dekamarkt.nl/boodschappen/producten/voorraadkast/oliÃ«n-sauzen-mixen/maggi-doseerjus-naturel-180-g/10\n",
      "  5. ID: 11, LastMod: 2024-10-03T00:55+02:00\n",
      "     URL: https://www.dekamarkt.nl/boodschappen/producten/dranken-sap-koffie-thee/aperitieven-mixen/tokkelroom-naturel-500-ml/11\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ FIXED SITEMAP PARSING FUNCTION\n",
    "\n",
    "def parse_product_urls_from_sitemap_fixed(sitemap_content: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse sitemap XML to extract product IDs and lastmod dates.\n",
    "    Fixed version for dekamarkt URL format.\n",
    "    \"\"\"\n",
    "    if not sitemap_content:\n",
    "        return []\n",
    "    \n",
    "    products = []\n",
    "    try:\n",
    "        root = ET.fromstring(sitemap_content)\n",
    "        \n",
    "        # Handle namespace\n",
    "        ns = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "        \n",
    "        for url_elem in root.findall('.//sitemap:url', ns):\n",
    "            loc_elem = url_elem.find('sitemap:loc', ns)\n",
    "            lastmod_elem = url_elem.find('sitemap:lastmod', ns)\n",
    "            \n",
    "            if loc_elem is not None:\n",
    "                url = loc_elem.text\n",
    "                # Extract product ID from URL (dekamarkt format: ends with /product-id)\n",
    "                # Example: https://www.dekamarkt.nl/boodschappen/producten/.../6\n",
    "                if '/producten/' in url:\n",
    "                    product_id_match = re.search(r'/(\\d+)$', url)\n",
    "                    \n",
    "                    if product_id_match:\n",
    "                        product_id = product_id_match.group(1)\n",
    "                        lastmod = lastmod_elem.text if lastmod_elem is not None else None\n",
    "                        \n",
    "                        # Parse lastmod date if available\n",
    "                        lastmod_date = None\n",
    "                        if lastmod:\n",
    "                            try:\n",
    "                                # Handle different date formats\n",
    "                                if 'T' in lastmod:\n",
    "                                    # Remove timezone info for simpler parsing\n",
    "                                    lastmod_clean = re.sub(r'[+-]\\d{2}:\\d{2}$', '', lastmod)\n",
    "                                    lastmod_date = datetime.fromisoformat(lastmod_clean)\n",
    "                                else:\n",
    "                                    lastmod_date = datetime.strptime(lastmod, '%Y-%m-%d')\n",
    "                            except Exception as e:\n",
    "                                print(f\"âš ï¸ Date parsing error for {lastmod}: {e}\")\n",
    "                                pass\n",
    "                        \n",
    "                        products.append({\n",
    "                            'product_id': product_id,\n",
    "                            'url': url,\n",
    "                            'lastmod': lastmod,\n",
    "                            'lastmod_date': lastmod_date\n",
    "                        })\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error parsing sitemap: {e}\")\n",
    "    \n",
    "    return products\n",
    "\n",
    "# Test the fixed function\n",
    "print(\"ğŸ”§ Testing fixed sitemap parsing...\")\n",
    "test_products = parse_product_urls_from_sitemap_fixed(sitemap_content)\n",
    "\n",
    "if test_products:\n",
    "    print(f\"âœ… Successfully parsed {len(test_products)} products\")\n",
    "    print(f\"\\nğŸ“‹ Sample products:\")\n",
    "    for i, product in enumerate(test_products[:5]):\n",
    "        print(f\"  {i+1}. ID: {product['product_id']}, LastMod: {product['lastmod']}\")\n",
    "        print(f\"     URL: {product['url']}\")\n",
    "else:\n",
    "    print(\"âŒ Still no products found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebd7e95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing Updated Dekamarkt Comprehensive Scraper\n",
      "============================================================\n",
      "ğŸ“‹ Starting test run with 3 products...\n",
      "ğŸ Starting Dekamarkt Comprehensive Scraper v2.0 (Fixed)\n",
      "============================================================\n",
      "ğŸ“¥ Fetching product sitemap...\n",
      "ğŸ” Parsing product URLs from sitemap...\n",
      "ğŸ“Š Found 11611 products in sitemap\n",
      "ğŸ”„ Force full scrape enabled - processing all products\n",
      "ğŸ¯ Limited to first 3 products for testing\n",
      "ğŸ›’ Scraping detailed information for 3 products...\n",
      "âœ… Scraping completed!\n",
      "ğŸ“Š Successfully scraped: 3 products\n",
      "âŒ Errors encountered: 0 products\n",
      "ğŸ“… Saved last update date: 2025-08-22 00:09:50.937537\n",
      "ğŸ’¾ Results saved with 3 products\n",
      "ğŸ“… Next scrape will check for products updated after: 2025-08-22 00:09:50.937537\n",
      "\n",
      "ğŸ“Š COMPREHENSIVE RESULTS OVERVIEW:\n",
      "Total products scraped: 3\n",
      "Columns available: 37\n",
      "\n",
      "ğŸ“‹ ALL AVAILABLE COLUMNS:\n",
      "   1. product_id\n",
      "   2. header_text\n",
      "   3. brand\n",
      "   4. packaging\n",
      "   5. is_weight_product\n",
      "   6. max_amount\n",
      "   7. department\n",
      "   8. webgroup\n",
      "   9. description\n",
      "  10. additional_description\n",
      "  11. normal_price\n",
      "  12. offer_price\n",
      "  13. is_single_use_plastic\n",
      "  14. product_number\n",
      "  15. start_date\n",
      "  16. end_date\n",
      "  17. offer_text_price_sign\n",
      "  18. offer_start_date\n",
      "  19. offer_end_date\n",
      "  20. disclaimer_start_date\n",
      "  21. disclaimer_end_date\n",
      "  22. main_image\n",
      "  23. all_images\n",
      "  24. image_count\n",
      "  25. logos\n",
      "  26. contact_name\n",
      "  27. contact_address\n",
      "  28. standard_packaging_unit\n",
      "  29. sold_or_prepared\n",
      "  30. nutritional_values\n",
      "  31. storage_instructions\n",
      "  32. cooking_instructions\n",
      "  33. instructions_for_use\n",
      "  34. ingredients\n",
      "  35. sitemap_url\n",
      "  36. lastmod\n",
      "  37. scraped_at\n",
      "\n",
      "ğŸ” SAMPLE PRODUCT DETAILS:\n",
      "Product ID: 6\n",
      "Name: Heineken Pilsener krat\n",
      "Brand: Heineken\n",
      "Price: â‚¬18.65\n",
      "Offer Price: â‚¬18.65\n",
      "Department: Dranken, sap, koffie & thee\n",
      "Description: PILSENER BIER. CAT I...\n",
      "Nutritional Info: Energie Kilojoules (kJ): 176; Energie KilocalorieÃ«n (kcal): 42; Vetten (g): 0;   - verzadigde vetzuren (g): 0; Koolhydraten (g): 3.2;   - suikers (g):...\n",
      "Ingredients: Water, GERSTEMOUT, hopextract...\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ UPDATED MAIN SCRAPING FUNCTION WITH FIXED PARSER\n",
    "\n",
    "def scrape_dekamarkt_comprehensive_v2(\n",
    "    store_id: int = STORE_ID,\n",
    "    delay_between_requests: float = 0.1,\n",
    "    max_products: int = None,\n",
    "    force_full_scrape: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Comprehensive Dekamarkt scraper using sitemap discovery and detailed product queries.\n",
    "    Updated with fixed sitemap parser.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ğŸ Starting Dekamarkt Comprehensive Scraper v2.0 (Fixed)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Fetch and parse sitemap\n",
    "    print(\"ğŸ“¥ Fetching product sitemap...\")\n",
    "    sitemap_content = fetch_sitemap(SITEMAP_URL)\n",
    "    if not sitemap_content:\n",
    "        print(\"âŒ Failed to fetch sitemap\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Step 2: Extract product URLs and lastmod dates (using fixed parser)\n",
    "    print(\"ğŸ” Parsing product URLs from sitemap...\")\n",
    "    products = parse_product_urls_from_sitemap_fixed(sitemap_content)\n",
    "    if not products:\n",
    "        print(\"âŒ No products found in sitemap\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"ğŸ“Š Found {len(products)} products in sitemap\")\n",
    "    \n",
    "    # Step 3: Filter by lastmod date (unless force_full_scrape)\n",
    "    if not force_full_scrape:\n",
    "        last_update = get_last_update_date()\n",
    "        products = filter_products_by_date(products, last_update)\n",
    "    else:\n",
    "        print(\"ğŸ”„ Force full scrape enabled - processing all products\")\n",
    "    \n",
    "    if not products:\n",
    "        print(\"âœ… No new products to scrape\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Step 4: Limit products for testing if specified\n",
    "    if max_products and len(products) > max_products:\n",
    "        products = products[:max_products]\n",
    "        print(f\"ğŸ¯ Limited to first {max_products} products for testing\")\n",
    "    \n",
    "    # Step 5: Scrape detailed product information\n",
    "    print(f\"ğŸ›’ Scraping detailed information for {len(products)} products...\")\n",
    "    scraped_products = []\n",
    "    errors = 0\n",
    "    \n",
    "    for i, product_info in enumerate(products, 1):\n",
    "        product_id = product_info['product_id']\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f\"ğŸ”„ Progress: {i}/{len(products)} products processed...\")\n",
    "        \n",
    "        # Fetch detailed product data\n",
    "        product_data = fetch_product_details(product_id, store_id)\n",
    "        if product_data:\n",
    "            # Extract comprehensive information\n",
    "            extracted_info = extract_comprehensive_product_info(product_data)\n",
    "            if extracted_info:\n",
    "                # Add sitemap metadata\n",
    "                extracted_info.update({\n",
    "                    'sitemap_url': product_info['url'],\n",
    "                    'lastmod': product_info['lastmod'],\n",
    "                    'scraped_at': datetime.now().isoformat()\n",
    "                })\n",
    "                scraped_products.append(extracted_info)\n",
    "            else:\n",
    "                errors += 1\n",
    "        else:\n",
    "            errors += 1\n",
    "        \n",
    "        # Rate limiting\n",
    "        if delay_between_requests > 0:\n",
    "            time.sleep(delay_between_requests)\n",
    "    \n",
    "    print(f\"âœ… Scraping completed!\")\n",
    "    print(f\"ğŸ“Š Successfully scraped: {len(scraped_products)} products\")\n",
    "    print(f\"âŒ Errors encountered: {errors} products\")\n",
    "    \n",
    "    # Step 6: Create DataFrame and save results\n",
    "    if scraped_products:\n",
    "        df = pd.DataFrame(scraped_products)\n",
    "        \n",
    "        # Save last update date\n",
    "        current_time = datetime.now()\n",
    "        save_last_update_date(current_time)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Results saved with {len(df)} products\")\n",
    "        print(f\"ğŸ“… Next scrape will check for products updated after: {current_time}\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"âŒ No products successfully scraped\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ğŸ§ª TEST THE UPDATED COMPREHENSIVE SCRAPER\n",
    "\n",
    "print(\"ğŸ§ª Testing Updated Dekamarkt Comprehensive Scraper\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with a small sample first\n",
    "print(\"ğŸ“‹ Starting test run with 3 products...\")\n",
    "\n",
    "# Run the comprehensive scraper\n",
    "df_comprehensive_v2 = scrape_dekamarkt_comprehensive_v2(\n",
    "    store_id=283,\n",
    "    delay_between_requests=0.2,  # 200ms delay to be safe\n",
    "    max_products=3,  # Test with just 3 products first\n",
    "    force_full_scrape=True  # Ignore date filtering for test\n",
    ")\n",
    "\n",
    "if not df_comprehensive_v2.empty:\n",
    "    print(f\"\\nğŸ“Š COMPREHENSIVE RESULTS OVERVIEW:\")\n",
    "    print(f\"Total products scraped: {len(df_comprehensive_v2)}\")\n",
    "    print(f\"Columns available: {len(df_comprehensive_v2.columns)}\")\n",
    "    \n",
    "    # Show all available columns\n",
    "    print(f\"\\nğŸ“‹ ALL AVAILABLE COLUMNS:\")\n",
    "    for i, col in enumerate(df_comprehensive_v2.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    \n",
    "    # Show sample product details\n",
    "    if len(df_comprehensive_v2) > 0:\n",
    "        print(f\"\\nğŸ” SAMPLE PRODUCT DETAILS:\")\n",
    "        sample = df_comprehensive_v2.iloc[0]\n",
    "        \n",
    "        # Key product information\n",
    "        print(f\"Product ID: {sample.get('product_id', 'N/A')}\")\n",
    "        print(f\"Name: {sample.get('header_text', 'N/A')}\")\n",
    "        print(f\"Brand: {sample.get('brand', 'N/A')}\")\n",
    "        print(f\"Price: â‚¬{sample.get('normal_price', 'N/A')}\")\n",
    "        print(f\"Offer Price: â‚¬{sample.get('offer_price', 'N/A') if sample.get('offer_price') else 'No offer'}\")\n",
    "        print(f\"Department: {sample.get('department', 'N/A')}\")\n",
    "        print(f\"Description: {str(sample.get('description', 'N/A'))[:100]}...\")\n",
    "        \n",
    "        # Nutritional info if available\n",
    "        if sample.get('nutritional_values'):\n",
    "            print(f\"Nutritional Info: {str(sample.get('nutritional_values', ''))[:150]}...\")\n",
    "        \n",
    "        # Ingredients if available\n",
    "        if sample.get('ingredients'):\n",
    "            print(f\"Ingredients: {str(sample.get('ingredients', ''))[:100]}...\")\n",
    "            \n",
    "else:\n",
    "    print(\"âŒ Test failed - no data retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bfb52829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Final Demonstration - Larger Sample\n",
      "============================================================\n",
      "ğŸ“‹ Running with 20 products and CSV export...\n",
      "ğŸ Starting Dekamarkt Comprehensive Scraper v2.0 (Fixed)\n",
      "============================================================\n",
      "ğŸ“¥ Fetching product sitemap...\n",
      "ğŸ” Parsing product URLs from sitemap...\n",
      "ğŸ“Š Found 11611 products in sitemap\n",
      "ğŸ”„ Force full scrape enabled - processing all products\n",
      "ğŸ¯ Limited to first 20 products for testing\n",
      "ğŸ›’ Scraping detailed information for 20 products...\n",
      "âœ… Scraping completed!\n",
      "ğŸ“Š Successfully scraped: 20 products\n",
      "âŒ Errors encountered: 0 products\n",
      "ğŸ“… Saved last update date: 2025-08-22 00:11:03.777622\n",
      "ğŸ’¾ Results saved with 20 products\n",
      "ğŸ“… Next scrape will check for products updated after: 2025-08-22 00:11:03.777622\n",
      "\n",
      "ğŸ“Š FINAL RESULTS SUMMARY:\n",
      "Total products scraped: 20\n",
      "Data richness: 37 columns per product\n",
      "ğŸ’¾ Saved 20 products to: dekamarkt_products_comprehensive_20250822_001103.csv\n",
      "\n",
      "ğŸ“ˆ DATA QUALITY METRICS:\n",
      "  header_text: 20/20 (100.0% complete)\n",
      "  brand: 20/20 (100.0% complete)\n",
      "  normal_price: 18/20 (90.0% complete)\n",
      "  description: 20/20 (100.0% complete)\n",
      "  nutritional_values: 10/20 (50.0% complete)\n",
      "  ingredients: 12/20 (60.0% complete)\n",
      "  main_image: 20/20 (100.0% complete)\n",
      "\n",
      "ğŸª PRODUCT CATEGORIES:\n",
      "  Kind & drogisterij: 8 products\n",
      "  Voorraadkast: 5 products\n",
      "  Dranken, sap, koffie & thee: 2 products\n",
      "  Brood, beleg & koek: 2 products\n",
      "  Aardappelen, groente & fruit: 1 products\n",
      "  Maaltijden, salades & tapas: 1 products\n",
      "  Huishoud & huisdieren: 1 products\n",
      "\n",
      "ğŸ·ï¸ TOP BRANDS:\n",
      "  Huggies: 4 products\n",
      "  Ambition: 2 products\n",
      "  Silvo: 2 products\n",
      "  Kotex: 2 products\n",
      "  Trida: 1 products\n",
      "\n",
      "ğŸ’° PRICE RANGE:\n",
      "  Min: â‚¬1.39\n",
      "  Max: â‚¬18.65\n",
      "  Average: â‚¬4.71\n",
      "\n",
      "ğŸ“„ SAMPLE PRODUCT NAMES:\n",
      "   1. Heineken Pilsener krat\n",
      "   2. Trida Zeepblok soft en sensitive\n",
      "   3. Bio+ Biologische rode bieten gekookt\n",
      "   4. Maggi Doseerjus naturel\n",
      "   5. Tokkelroom Naturel\n",
      "   6. Ambition Gembersiroop\n",
      "   7. Ambition Chinese bakgember\n",
      "   8. Flemmings Broodje frikandel\n",
      "   9. Whiskas Saus rundvlees\n",
      "  10. Sudocrem CrÃ¨me hypo-allergeen\n",
      "\n",
      "âœ… SUCCESS! Comprehensive scraper ready for production use.\n",
      "ğŸ“ Data saved to: dekamarkt_products_comprehensive_20250822_001103.csv\n",
      "ğŸ”„ Incremental updates: Enabled with lastmod date tracking\n",
      "ğŸ“Š Total discoverable products: 11,611\n",
      "\n",
      "============================================================\n",
      "ğŸ Dekamarkt Comprehensive Scraper v2.0 - READY FOR PRODUCTION!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ FINAL DEMONSTRATION - LARGER SAMPLE WITH CSV EXPORT\n",
    "\n",
    "print(\"ğŸ¯ Final Demonstration - Larger Sample\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run with more products to show scalability\n",
    "print(\"ğŸ“‹ Running with 20 products and CSV export...\")\n",
    "\n",
    "# Run the comprehensive scraper with more products\n",
    "df_final = scrape_dekamarkt_comprehensive_v2(\n",
    "    store_id=283,\n",
    "    delay_between_requests=0.15,  # 150ms delay for good balance\n",
    "    max_products=20,  # Test with 20 products\n",
    "    force_full_scrape=True  # Ignore date filtering for demo\n",
    ")\n",
    "\n",
    "if not df_final.empty:\n",
    "    print(f\"\\nğŸ“Š FINAL RESULTS SUMMARY:\")\n",
    "    print(f\"Total products scraped: {len(df_final)}\")\n",
    "    print(f\"Data richness: {len(df_final.columns)} columns per product\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_filename = save_to_csv(df_final)\n",
    "    \n",
    "    # Show data quality metrics\n",
    "    print(f\"\\nğŸ“ˆ DATA QUALITY METRICS:\")\n",
    "    \n",
    "    key_fields = ['header_text', 'brand', 'normal_price', 'description', \n",
    "                 'nutritional_values', 'ingredients', 'main_image']\n",
    "    \n",
    "    for field in key_fields:\n",
    "        if field in df_final.columns:\n",
    "            non_null_count = df_final[field].notna().sum()\n",
    "            percentage = (non_null_count / len(df_final)) * 100\n",
    "            print(f\"  {field}: {non_null_count}/{len(df_final)} ({percentage:.1f}% complete)\")\n",
    "    \n",
    "    # Show product categories\n",
    "    if 'department' in df_final.columns:\n",
    "        dept_counts = df_final['department'].value_counts()\n",
    "        print(f\"\\nğŸª PRODUCT CATEGORIES:\")\n",
    "        for dept, count in dept_counts.head(10).items():\n",
    "            print(f\"  {dept}: {count} products\")\n",
    "    \n",
    "    # Show brand distribution\n",
    "    if 'brand' in df_final.columns:\n",
    "        brand_counts = df_final['brand'].value_counts()\n",
    "        print(f\"\\nğŸ·ï¸ TOP BRANDS:\")\n",
    "        for brand, count in brand_counts.head(5).items():\n",
    "            if pd.notna(brand):\n",
    "                print(f\"  {brand}: {count} products\")\n",
    "    \n",
    "    # Show price range\n",
    "    if 'normal_price' in df_final.columns:\n",
    "        prices = df_final['normal_price'].dropna()\n",
    "        if not prices.empty:\n",
    "            print(f\"\\nğŸ’° PRICE RANGE:\")\n",
    "            print(f\"  Min: â‚¬{prices.min():.2f}\")\n",
    "            print(f\"  Max: â‚¬{prices.max():.2f}\")\n",
    "            print(f\"  Average: â‚¬{prices.mean():.2f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“„ SAMPLE PRODUCT NAMES:\")\n",
    "    for i, name in enumerate(df_final['header_text'].head(10), 1):\n",
    "        if pd.notna(name):\n",
    "            print(f\"  {i:2d}. {name}\")\n",
    "            \n",
    "    print(f\"\\nâœ… SUCCESS! Comprehensive scraper ready for production use.\")\n",
    "    print(f\"ğŸ“ Data saved to: {csv_filename}\")\n",
    "    print(f\"ğŸ”„ Incremental updates: Enabled with lastmod date tracking\")\n",
    "    print(f\"ğŸ“Š Total discoverable products: 11,611\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Final test failed\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ Dekamarkt Comprehensive Scraper v2.0 - READY FOR PRODUCTION!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b89718f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SITEMAP_URL: https://www.dekamarkt.nl/products-sitemap.xml\n",
      "Testing sitemap access from notebook...\n",
      "Status: 403\n",
      "Error: 403\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"SITEMAP_URL:\", SITEMAP_URL)\n",
    "print(\"Testing sitemap access from notebook...\")\n",
    "\n",
    "import requests\n",
    "try:\n",
    "    resp = requests.get(SITEMAP_URL, timeout=30)\n",
    "    print(f\"Status: {resp.status_code}\")\n",
    "    if resp.status_code == 200:\n",
    "        print(f\"Content length: {len(resp.text)}\")\n",
    "        print(\"Sitemap accessible!\")\n",
    "    else:\n",
    "        print(f\"Error: {resp.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
